{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Pipeline API and ML workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:03:18.565235Z",
     "start_time": "2021-06-20T18:03:18.551310Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, IDF, CountVectorizer, VectorAssembler, Word2Vec\n",
    "from pyspark.sql.types import StructType, StringType, ArrayType, IntegerType\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from pyspark.sql.functions import mean, col, udf, regexp_replace, lower, size, monotonically_increasing_id\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:01:55.588651Z",
     "start_time": "2021-06-20T18:01:55.575770Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipeline API and spark SQL\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:01:57.264722Z",
     "start_time": "2021-06-20T18:01:57.239309Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType().add(\"tweets\",StringType(),True)\n",
    "\n",
    "\n",
    "df = spark.read.schema(schema).csv(\"tweets.json\", header=True)\n",
    "df = df.sample(withReplacement=False, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T12:16:14.988327Z",
     "start_time": "2021-06-19T12:16:14.985202Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.collect()[3]\n",
    "#df.select(\"tweets\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T15:59:28.485204Z",
     "start_time": "2021-06-20T15:59:28.344701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"To come in and play against England my first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@TunnockCup It's absolutely class. They could ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I can\\u2019t quite believe it but we made it t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Amakhosi4Life We are Kaizer Chiefs @KaizerChi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you are a 5 star recruit in football you sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@jumbleofideas @paldhous @PeterHotez I searche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True loneliness is a solitary comet flying thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"don't buy wine with my money and then give it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How might we invest in equitable collaborative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\u2b55 Admission Alert \\u2b55  &amp;gt;&amp;gt;&amp;gt;&amp;gt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@noellaRH I can't wait to someday be on the op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@HeRealistic @SentaiWRLD @sonicpop54 @Edelgaym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#30yrsago another fantastic song by the great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>And there...it...all...goes...again. #Sandra #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sandra.c was just registered! Register your ow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweets\n",
       "0   \"To come in and play against England my first ...\n",
       "1   @TunnockCup It's absolutely class. They could ...\n",
       "2   I can\\u2019t quite believe it but we made it t...\n",
       "3   #Amakhosi4Life We are Kaizer Chiefs @KaizerChi...\n",
       "4   If you are a 5 star recruit in football you sh...\n",
       "5   @jumbleofideas @paldhous @PeterHotez I searche...\n",
       "6   True loneliness is a solitary comet flying thr...\n",
       "7   \"don't buy wine with my money and then give it...\n",
       "8   How might we invest in equitable collaborative...\n",
       "9   \\u2b55 Admission Alert \\u2b55  &gt;&gt;&gt;&gt...\n",
       "10  @noellaRH I can't wait to someday be on the op...\n",
       "11  @HeRealistic @SentaiWRLD @sonicpop54 @Edelgaym...\n",
       "12  #30yrsago another fantastic song by the great ...\n",
       "13  And there...it...all...goes...again. #Sandra #...\n",
       "14  sandra.c was just registered! Register your ow..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing without pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T15:59:34.653592Z",
     "start_time": "2021-06-20T15:59:34.032423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=['\"to', 'ome', 'lay', 'englan', 'ir', 'game,', 'ro\"', 'billy', 'gilmor', 'give', 'reaion', 'imreive', 'eb', \"solan'\", 'raw', 'wi', 'englan', 'eng', 'sco', 'engsco', 'euro', 'taranarmy', 'treelion', 'oqprsnq'], tf_features=SparseVector(190, {8: 1.0, 9: 1.0, 10: 2.0, 27: 1.0, 35: 1.0, 36: 1.0, 47: 1.0, 53: 1.0, 56: 1.0, 61: 1.0, 63: 1.0, 67: 1.0, 70: 1.0, 79: 1.0, 102: 1.0, 103: 1.0, 106: 1.0, 112: 1.0, 125: 1.0, 140: 1.0, 150: 1.0, 156: 1.0, 168: 1.0}), tf-idf_features=SparseVector(190, {8: 1.3863, 9: 1.674, 10: 4.1589, 27: 1.674, 35: 2.0794, 36: 2.0794, 47: 2.0794, 53: 2.0794, 56: 2.0794, 61: 2.0794, 63: 2.0794, 67: 2.0794, 70: 2.0794, 79: 2.0794, 102: 2.0794, 103: 2.0794, 106: 2.0794, 112: 2.0794, 125: 2.0794, 140: 2.0794, 150: 2.0794, 156: 2.0794, 168: 2.0794})),\n",
       " Row(words=['tnnokc', \"i'\", 'abolely', 'la', 'tey', 'ol', 'ave', 'enjoye', 'ooball', 'like', 'normal', 'eole', 'ala'], tf_features=SparseVector(190, {3: 1.0, 5: 1.0, 11: 1.0, 12: 1.0, 15: 1.0, 19: 1.0, 25: 1.0, 72: 1.0, 78: 1.0, 147: 1.0, 151: 1.0, 170: 1.0, 186: 1.0}), tf-idf_features=SparseVector(190, {3: 1.1632, 5: 1.3863, 11: 1.674, 12: 1.674, 15: 1.674, 19: 1.674, 25: 1.674, 72: 2.0794, 78: 2.0794, 147: 2.0794, 151: 2.0794, 170: 2.0794, 186: 2.0794})),\n",
       " Row(words=['an19', 'qie', 'believe', 'mae', 'rog', 'wio', 'ingle', 'rnk', 'ooball', 'e1'], tf_features=SparseVector(190, {5: 1.0, 30: 1.0, 84: 1.0, 89: 1.0, 91: 1.0, 94: 1.0, 131: 1.0, 145: 1.0, 160: 1.0, 164: 1.0}), tf-idf_features=SparseVector(190, {5: 1.3863, 30: 1.674, 84: 2.0794, 89: 2.0794, 91: 2.0794, 94: 2.0794, 131: 2.0794, 145: 2.0794, 160: 2.0794, 164: 2.0794})),\n",
       " Row(words=['amakoi4lie', 'kaizer', 'cie', 'kaizercie', 'kaizer', 'te', 'mo', 'el', 'eam', 'iory', 'arian', 'ooballte', 'cup', 'seialitoay', 'rial', 'game', 'migy', 'amakoi', 'viorio', 'te', 'glamor', 'boy,', 'reay', 'ojgynyoby'], tf_features=SparseVector(190, {2: 2.0, 23: 2.0, 29: 1.0, 31: 1.0, 60: 1.0, 65: 1.0, 68: 1.0, 71: 1.0, 75: 1.0, 76: 1.0, 88: 1.0, 92: 1.0, 99: 1.0, 101: 1.0, 109: 1.0, 122: 1.0, 123: 1.0, 141: 1.0, 146: 1.0, 159: 1.0, 173: 1.0, 176: 1.0}), tf-idf_features=SparseVector(190, {2: 2.7726, 23: 4.1589, 29: 1.674, 31: 2.0794, 60: 2.0794, 65: 2.0794, 68: 2.0794, 71: 2.0794, 75: 2.0794, 76: 2.0794, 88: 2.0794, 92: 2.0794, 99: 2.0794, 101: 2.0794, 109: 2.0794, 122: 2.0794, 123: 2.0794, 141: 2.0794, 146: 2.0794, 159: 2.0794, 173: 2.0794, 176: 2.0794})),\n",
       " Row(words=['yo', 'ar', 'reri', 'ooball', 'yo', 'ol', 'go', 'etsu'], tf_features=SparseVector(190, {0: 2.0, 3: 1.0, 5: 1.0, 134: 1.0, 138: 1.0, 154: 1.0, 165: 1.0}), tf-idf_features=SparseVector(190, {0: 2.7726, 3: 1.1632, 5: 1.3863, 134: 2.0794, 138: 2.0794, 154: 2.0794, 165: 2.0794})),\n",
       " Row(words=['jmbleoiea', 'alo', 'peerhoez', 'eare', \"an'\", 'eii', 'aa', 'abo', 'j&am;j', 'dela', 'ol', 'igniian', 'el', 'keeing', 'inee', 'eole', 'oial,', 'og,', 'oer', 'varian', '(i', 'yo', 'wan', 'ak', 'mrna', 'vax', 'i,', 'onl', 'yor', 'oor)'], tf_features=SparseVector(190, {0: 1.0, 3: 1.0, 16: 1.0, 19: 1.0, 22: 1.0, 29: 1.0, 33: 1.0, 43: 1.0, 45: 1.0, 58: 1.0, 62: 1.0, 69: 1.0, 74: 1.0, 86: 1.0, 93: 1.0, 97: 1.0, 98: 1.0, 118: 1.0, 124: 1.0, 126: 1.0, 127: 1.0, 142: 1.0, 144: 1.0, 149: 1.0, 155: 1.0, 162: 1.0, 163: 1.0, 180: 1.0, 183: 1.0, 188: 1.0}), tf-idf_features=SparseVector(190, {0: 1.3863, 3: 1.1632, 16: 1.674, 19: 1.674, 22: 1.674, 29: 1.674, 33: 2.0794, 43: 2.0794, 45: 2.0794, 58: 2.0794, 62: 2.0794, 69: 2.0794, 74: 2.0794, 86: 2.0794, 93: 2.0794, 97: 2.0794, 98: 2.0794, 118: 2.0794, 124: 2.0794, 126: 2.0794, 127: 2.0794, 142: 2.0794, 144: 2.0794, 149: 2.0794, 155: 2.0794, 162: 2.0794, 163: 2.0794, 180: 2.0794, 183: 2.0794, 188: 2.0794})),\n",
       " Row(words=['tre', 'loneline', 'oliary', 'ome', 'lying', 'rog', 'ae', 'noingne', 'abole', 'zero', 'te', 'imoibiliy', 'anyone', 'ee', 'yo,', 'anyone', 'aroa', 'yo', 'te', 'ol', 'ilene', 'la', 'oan', 'year', 'yo', 'know', 'wa', \"i'\", 'like', 'one', 'know', 'exe', 'yo,', 'cya'], tf_features=SparseVector(190, {0: 2.0, 1: 1.0, 2: 2.0, 3: 1.0, 11: 1.0, 12: 1.0, 15: 1.0, 17: 2.0, 21: 2.0, 26: 1.0, 27: 1.0, 28: 2.0, 30: 1.0, 40: 1.0, 41: 1.0, 83: 1.0, 90: 1.0, 96: 1.0, 110: 1.0, 111: 1.0, 115: 1.0, 117: 1.0, 136: 1.0, 157: 1.0, 166: 1.0, 169: 1.0, 178: 1.0, 181: 1.0, 185: 1.0}), tf-idf_features=SparseVector(190, {0: 2.7726, 1: 1.1632, 2: 2.7726, 3: 1.1632, 11: 1.674, 12: 1.674, 15: 1.674, 17: 4.1589, 21: 4.1589, 26: 1.674, 27: 1.674, 28: 4.1589, 30: 1.674, 40: 2.0794, 41: 2.0794, 83: 2.0794, 90: 2.0794, 96: 2.0794, 110: 2.0794, 111: 2.0794, 115: 2.0794, 117: 2.0794, 136: 2.0794, 157: 2.0794, 166: 2.0794, 169: 2.0794, 178: 2.0794, 181: 2.0794, 185: 2.0794})),\n",
       " Row(words=['\"on\\'', 'wine', 'wi', 'money', 'en', 'give', 'ary\"'], tf_features=SparseVector(190, {8: 1.0, 9: 1.0, 107: 1.0, 121: 1.0, 153: 1.0, 167: 1.0, 187: 1.0}), tf-idf_features=SparseVector(190, {8: 1.3863, 9: 1.674, 107: 2.0794, 121: 2.0794, 153: 2.0794, 167: 2.0794, 187: 2.0794})),\n",
       " Row(words=['mig', 'inve', 'eqiable', 'ollaboraive', 'ae', 'njea'], tf_features=SparseVector(190, {1: 1.0, 100: 1.0, 129: 1.0, 130: 1.0, 137: 1.0, 175: 1.0}), tf-idf_features=SparseVector(190, {1: 1.1632, 100: 2.0794, 129: 2.0794, 130: 2.0794, 137: 2.0794, 175: 2.0794})),\n",
       " Row(words=['b55', 'amiion', 'aler', 'b55', '&g;&g;&g;&g;inie', 'sae', 'tenology', 'ilamaba', '&l;&l;&l;&l;', 'te', 'inie', 'sae', 'tenology', '(ist)', 'bli', 'niveriy', 'loae', 'ilamaba,', 'pakian', 'ner', 'aminiraion', 'sae', 'uer', 'amoere', 'reear', 'commiion', 'ozevvkjt'], tf_features=SparseVector(190, {2: 1.0, 7: 3.0, 18: 2.0, 24: 2.0, 32: 1.0, 39: 1.0, 44: 1.0, 51: 1.0, 55: 1.0, 57: 1.0, 59: 1.0, 66: 1.0, 80: 1.0, 95: 1.0, 113: 1.0, 116: 1.0, 133: 1.0, 139: 1.0, 143: 1.0, 148: 1.0, 161: 1.0, 174: 1.0, 177: 1.0}), tf-idf_features=SparseVector(190, {2: 1.3863, 7: 6.2383, 18: 4.1589, 24: 4.1589, 32: 2.0794, 39: 2.0794, 44: 2.0794, 51: 2.0794, 55: 2.0794, 57: 2.0794, 59: 2.0794, 66: 2.0794, 80: 2.0794, 95: 2.0794, 113: 2.0794, 116: 2.0794, 133: 2.0794, 139: 2.0794, 143: 2.0794, 148: 2.0794, 161: 2.0794, 174: 2.0794, 177: 2.0794})),\n",
       " Row(words=['noellarh', \"an'\", 'wai', 'omeay', 'oen', 'roa,', 'navigaing', 'ae', 'beween', 'ere', 'ere', 'lae', 'rien', 'enne', 'awai', 'wi', 'eir', 'oen', 'arm', 'eir', 'oen', 'ear'], tf_features=SparseVector(190, {1: 1.0, 4: 3.0, 6: 2.0, 8: 1.0, 16: 1.0, 20: 2.0, 46: 1.0, 49: 1.0, 52: 1.0, 54: 1.0, 64: 1.0, 77: 1.0, 81: 1.0, 87: 1.0, 114: 1.0, 132: 1.0, 172: 1.0, 184: 1.0}), tf-idf_features=SparseVector(190, {1: 1.1632, 4: 6.2383, 6: 3.348, 8: 1.3863, 16: 1.674, 20: 4.1589, 46: 2.0794, 49: 2.0794, 52: 2.0794, 54: 2.0794, 64: 2.0794, 77: 2.0794, 81: 2.0794, 87: 2.0794, 114: 2.0794, 132: 2.0794, 172: 2.0794, 184: 2.0794})),\n",
       " Row(words=['herealii', 'senaiwrld', 'onio54', 'eelgaymer9', 'tere19', 'ae', 'rn,', 'barely', 'ave', 'ae', 'walk'], tf_features=SparseVector(190, {1: 2.0, 25: 1.0, 38: 1.0, 73: 1.0, 104: 1.0, 105: 1.0, 108: 1.0, 119: 1.0, 158: 1.0, 189: 1.0}), tf-idf_features=SparseVector(190, {1: 2.3263, 25: 1.674, 38: 2.0794, 73: 2.0794, 104: 2.0794, 105: 2.0794, 108: 2.0794, 119: 2.0794, 158: 2.0794, 189: 2.0794})),\n",
       " Row(words=['yrago', 'anoer', 'anai', 'ong', 'grea', 'sanra', 'orimrnkjm', 'lilegirl', 'neveren'], tf_features=SparseVector(190, {14: 1.0, 34: 1.0, 37: 1.0, 42: 1.0, 48: 1.0, 120: 1.0, 171: 1.0, 179: 1.0, 182: 1.0}), tf-idf_features=SparseVector(190, {14: 1.674, 34: 2.0794, 37: 2.0794, 42: 2.0794, 48: 2.0794, 120: 2.0794, 171: 2.0794, 179: 2.0794, 182: 2.0794})),\n",
       " Row(words=['ereiallgoeagain', 'sanra', 'preyorlk'], tf_features=SparseVector(190, {14: 1.0, 135: 1.0, 152: 1.0}), tf-idf_features=SparseVector(190, {14: 1.674, 135: 2.0794, 152: 2.0794})),\n",
       " Row(words=['anra', 'wa', 'regiere!', 'regier', 'yor', 'omain', 'ere', 'ouzv5tqr', 'anra'], tf_features=SparseVector(190, {6: 1.0, 13: 2.0, 22: 1.0, 26: 1.0, 50: 1.0, 82: 1.0, 85: 1.0, 128: 1.0}), tf-idf_features=SparseVector(190, {6: 1.674, 13: 4.1589, 22: 1.674, 26: 1.674, 50: 2.0794, 82: 2.0794, 85: 2.0794, 128: 2.0794}))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove irrelevant characters, numbers, links and punctuation marks\n",
    "regex = \"\\\\ud83c\\\\udc00-\\\\ud83c\\\\udfff]|[\\\\ud83d\\\\udc00-\\\\ud83d\\\\udfff]|[\\\\u2600-\\\\u27ff]|[^#]|[^@*$]|[^https?:\\/\\/.*[\\r\\n]*]\"\n",
    "\n",
    "\n",
    "df_clean = df.select(regexp_replace('tweets', r'[0-9]{5,}', '').alias('tweets'))\n",
    "#df_clean = df_clean.select('tweets', (lower(regexp_replace('tweets', \"[\\s+[a-zA-Z]\\s+][\\^[a-zA-Z]\\s+]\", \"\")).alias('tweets2')))\n",
    "df_clean = df_clean.select(\"tweets\", f.translate(f.col(\"tweets\"), regex, \"\").alias(\"text\"))\n",
    "\n",
    "# Tokenizing\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='tokens')\n",
    "df_tokens = tokenizer.transform(df_clean).select('tokens')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='words')\n",
    "df_words = remover.transform(df_tokens).select('words')\n",
    "\n",
    "# Remove words of length less than 1\n",
    "len_udf = udf(lambda tokens: [token for token in tokens if len(token) > 1], ArrayType(StringType()))\n",
    "\n",
    "df_final = df_words.withColumn(\"words\", len_udf(\"words\")).select('words')\n",
    "\n",
    "#df_tokens.collect()[3]\n",
    "\n",
    "# Computing tf-idf\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"tf_features\")\n",
    "cvModel = cv.fit(df_final)\n",
    "tfDf = cvModel.transform(df_final)\n",
    "\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"tf-idf_features\")\n",
    "idfModel = idf.fit(tfDf)\n",
    "tfidfDf = idfModel.transform(tfDf)\n",
    "\n",
    "tfidfDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing through a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:02:04.381509Z",
     "start_time": "2021-06-20T18:02:04.365533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Transformer\n",
    "\n",
    "class CustomCleaner(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which removes punctuations, undefined chars, numerals, links\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, regex=\"\"):\n",
    "        self.regex =\"\\\\ud83c\\\\udc00-\\\\ud83c\\\\udfff]|[\\\\ud83d\\\\udc00-\\\\ud83d\\\\udfff]|[\\\\u2600-\\\\u27ff]|[^#]|[^@*$]|[^https?:\\/\\/.*[\\r\\n]*]\"\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        df_clean = df.select(regexp_replace('tweets', r'[0-9]{5,}', '').alias('tweets'))\n",
    "        df_clean = df_clean.select(\"tweets\", f.translate(f.col(\"tweets\"), regex, \"\").alias('tweets2'))\n",
    "        \n",
    "        return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:02:10.766179Z",
     "start_time": "2021-06-20T18:02:10.750547Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        tokenize = udf(lambda tokens: word_tokenize(tokens), ArrayType(StringType()))\n",
    "        df = df.withColumn(\"words\", tokenize(\"tweets2\"))\n",
    "    \n",
    "        return df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:02:12.424723Z",
     "start_time": "2021-06-20T18:02:12.418426Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomStopwordRemover(Transformer):\n",
    "    \"\"\"\n",
    "    A custom stopword remover\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        remover = udf(lambda tokens: [token for token in tokens if token not in [stopwords]], ArrayType(StringType()))\n",
    "        df = df.withColumn(\"tokens\", remover(\"words\"))\n",
    "        \n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:02:14.179016Z",
     "start_time": "2021-06-20T18:02:14.173789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Transformer\n",
    "\n",
    "class CustomFilter(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which removes words having length less than 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        len_udf = udf(lambda tokens: [token for token in tokens if len(token) > 1], ArrayType(StringType()))\n",
    "        df = df.withColumn(\"filtered\", len_udf(\"tokens\"))\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T19:22:55.441168Z",
     "start_time": "2021-06-20T19:22:55.427541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Transformer for tf\n",
    "\n",
    "class CustomTF(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which computes TF-IDF\n",
    "    The code has been inspired from here: https://towardsdatascience.com/tf-idf-calculation-using-map-reduce-algorithm-in-pyspark-e89b5758e64c\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        #df = df.withColumn(\"tweet_id\", monotonically_increasing_id())\n",
    "\n",
    "        all_df = df.select('filtered').collect()\n",
    "\n",
    "        allll = [(i,all_df[i].filtered) for i in range(0,10)]\n",
    "\n",
    "\n",
    "        lines = sc.parallelize(allll)\n",
    "\n",
    "        map1 = lines.flatMap(lambda x: [((x[0],i),1) for i in x[1]])\n",
    "\n",
    "        reduce = map1.reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "        tf = reduce.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "\n",
    "        map3 = reduce.map(lambda x: (x[0][1],(x[0][0],x[1],1)))\n",
    "\n",
    "        map4 = map3.map(lambda x:(x[0],x[1][2]))\n",
    "\n",
    "        reduce2 = map4.reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "        idf = reduce2.map(lambda x: (x[0],math.log10(len(allll)/x[1])))\n",
    "\n",
    "\n",
    "        rdd = tf.join(idf)\n",
    "\n",
    "\n",
    "\n",
    "        rdd = rdd.map(lambda x: (x[1][0][0],(x[0],x[1][0][1],x[1][1],x[1][0][1]*x[1][1]))).sortByKey()\n",
    "        rdd = rdd.map(lambda x: (x[0],x[1][0],x[1][1],x[1][2],x[1][3]))\n",
    "        tf = rdd.toDF([\"DocumentId\",\"Token\",\"TF\",\"IDF\",\"TF-IDF\"])\n",
    "\n",
    "        \n",
    "        return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:13:19.569185Z",
     "start_time": "2021-06-20T18:13:17.865105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+------------------+------------------+\n",
      "|DocumentId|    Token| TF|               IDF|            TF-IDF|\n",
      "+----------+---------+---+------------------+------------------+\n",
      "|         0| JameToe1|  1|               1.0|               1.0|\n",
      "|         0|       we|  1|0.6989700043360189|0.6989700043360189|\n",
      "|         0|      win|  1|               1.0|               1.0|\n",
      "|         0|   ooball|  1|0.3010299956639812|0.3010299956639812|\n",
      "|         0|       Di|  1|               1.0|               1.0|\n",
      "|         1|     noie|  1|               1.0|               1.0|\n",
      "|         1|    Raial|  1|               1.0|               1.0|\n",
      "|         1|    Brexi|  1|               1.0|               1.0|\n",
      "|         1|       An|  1|               1.0|               1.0|\n",
      "|         1|     oern|  1|               1.0|               1.0|\n",
      "|         1|   ooball|  1|0.3010299956639812|0.3010299956639812|\n",
      "|         1|     orer|  1|               1.0|               1.0|\n",
      "|         1|       in|  2|0.3010299956639812|0.6020599913279624|\n",
      "|         1|       yo|  1|               1.0|               1.0|\n",
      "|         1|Naionalim|  1|               1.0|               1.0|\n",
      "|         1|      lea|  1|0.6989700043360189|0.6989700043360189|\n",
      "|         1|   Toenam|  1|               1.0|               1.0|\n",
      "|         1|  menaliy|  1|               1.0|               1.0|\n",
      "|         1|      eir|  2|0.6989700043360189|1.3979400086720377|\n",
      "|         1|   Arenal|  1|               1.0|               1.0|\n",
      "+----------+---------+---+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaner = CustomCleaner()\n",
    "tokenizer = CustomTokenizer()\n",
    "remover = CustomStopwordRemover()\n",
    "final = CustomFilter()\n",
    "tfidf = CustomTF()\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[cleaner, tokenizer, remover, final, tfidf])\n",
    "\n",
    "# Send data through the pipeline\n",
    "model = pipeline.fit(df)\n",
    "results = model.transform(df)\n",
    "\n",
    "results.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 -- Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:27:56.535245Z",
     "start_time": "2021-06-20T18:27:56.113357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [@jamestudhope1, win, football?, \\ud83d\\ude2f] => \n",
      "Vector: [0.010278148984070867,-0.014843338401988149,-0.021595153375528753,0.012239520438015461,-0.0008178940042853355,-0.02115490846335888,0.033132049953565,-0.01352766321360832,-0.001545988954603672,-0.0016770113725215197]\n",
      "\n",
      "Text: [@sazmeister88, notice, least, 75%, brexit, trolls, bio, arsenal, tottenham, number, southern, english, football, teams., , mentality., racial, hatred,, football, nationalism, =, brexit, supporter.] => \n",
      "Vector: [0.0013939971509187117,-0.0007995152238594449,-0.004296151420061031,0.004122104738717494,-0.01219565887004137,3.765797768921956e-05,-0.010953001455282387,-0.00827415520325303,-0.01415190027024516,8.47697247872534e-05]\n",
      "\n",
      "Text: [@tasaflteam, comments, clearly, show, conflict, interest,, club, presidents, involved, process., narrow, minded,, uneducated, comments, reek, self, interests, fear., decision, needs, made, facts,, merit, future, tas, football.] => \n",
      "Vector: [-0.0024091002792724445,-0.004750775598670142,-0.0018509344663470984,0.0072606360548749,-0.009377213320336664,-0.001588006217319232,0.006482065861256651,0.01159431356847143,-0.010080686940525014,-0.00448008339243153]\n",
      "\n",
      "Text: [@biscaynebaysc, @zephyros21, @savethemanatee, @lowerleagueusa, @lowerleagueecup, @cansoccerhist, @cookstreetutd, @himmarsheefc, @thebreakersafc, @mplscitysc, yellow,, sure!] => \n",
      "Vector: [-0.00026258071496461827,0.00838894967455417,-0.009788876554618279,-0.016457950036662318,-0.0291021613132519,0.015233496281628806,-0.0027380551521976786,0.014500403000662724,0.00028474284529996413,-0.006120088888565078]\n",
      "\n",
      "Text: [@durrantmark, sadly,, one, byu, football, 20, point, lead, vs, utah, smh] => \n",
      "Vector: [-0.0006740088521672244,-0.008609770061659881,-0.008532501503147863,0.0056757079860703516,-0.0009825402362780137,0.008282255402512172,0.004676623820242557,0.0007261557250537655,-0.008449169087477705,0.0004495964808897539]\n",
      "\n",
      "Text: [@mthemreds, football, meant, pleasure,, complete, gash.] => \n",
      "Vector: [0.006707613138132729,-0.0031640619660417237,-0.009171563220055152,0.016868045097605013,-0.011898818270613749,0.016945212458570797,-0.0047041375655680895,0.010092120772848526,-0.009263711826254923,0.010332923227300245]\n",
      "\n",
      "Text: [@rfborthwick, scotland, fans, see, morning, alcohol, wears, off,, exactly, bragging, rights, \\ud83d\\ude02, , https://t.co/uowkukuqab] => \n",
      "Vector: [0.00962131711587842,-0.0021996250475889868,-0.015374296877001013,0.011701229593849607,-0.001286018839372056,0.008066916522303862,-0.01093807629409379,0.002305443481808262,-0.02186569052615336,-0.0021316599366920336]\n",
      "\n",
      "Text: [@jff_football, @paulhall22, @paulhall22, amazing, hally!, good, luck, new, role\\ud83d\\udd25\\ud83d\\udd25\\ud83d\\udd25] => \n",
      "Vector: [0.028844252435697448,0.02667750066353215,-0.007403352039141787,-0.0001314130818678273,-0.014484521932899952,-0.019372234146835074,0.0037594593595713377,-0.011051013393120633,-0.0029465890386038353,0.0037016091195659503]\n",
      "\n",
      "Text: [@paulharper82, say, surprised,, never, play, attractive, football, score, many, goals, major, tournaments., recently, anyway.] => \n",
      "Vector: [-0.011655733554757066,0.004566992134121911,-0.013579203115243996,0.005262182227202824,0.0014566080644726753,0.011976368392684629,-0.013421666342765093,0.004312671338474111,-0.0007278448902070522,-0.002308103921157973]\n",
      "\n",
      "Text: [@onan2019, @anitwtls, full, metal, alchemist, shit] => \n",
      "Vector: [0.004931943180660406,0.004248060596485932,-0.005385887809097767,0.005794236979757746,0.0009872383282830317,0.0005532068898901343,-0.007609894964843988,-0.009636174887418747,-0.015236621567358572,0.00669609165440003]\n",
      "\n",
      "Text: [@mojackmarine, people, tend, compartmentalize., usually, results, incredibly, tunnel, vision.] => \n",
      "Vector: [-0.016526343093978033,0.0035521853197779917,-0.003805479241742028,-0.005287209389886508,-0.002572057923922936,-0.0027876139308015504,-0.0007619255532821019,0.014177868349684609,-0.0040205130353569984,-0.010810535142405165]\n",
      "\n",
      "Text: [@ladbible, long, list....but, 'space, bound', always, touchy, one, \\u2728\\ud83c\\udfb6\\ud83c\\udfb6\\ud83c\\udfb6\\ud83c\\udfb6\\ud83d\\udc95] => \n",
      "Vector: [0.005674665981334531,-0.007951209269877937,-0.006176140625029802,-0.0056887020149992565,-0.010259422033818232,0.0014316049669610544,0.0064957815532883005,0.004164666661785709,0.0012425829966862996,0.01896236619601647]\n",
      "\n",
      "Text: [\\u2b55, admission, alert, \\u2b55, , &gt;&gt;&gt;&gt;institute, space, technology, islamabad, &lt;&lt;&lt;&lt;, , institute, space, technology, (ist), public, university, located, islamabad,, pakistan, administration, space, upper, atmosphere, research, commission., , https://t.co/zev7v7pkjt] => \n",
      "Vector: [-0.004713506569844737,0.003085500101276141,-0.004424506216309965,0.01328030345030129,0.007313938840525225,0.004014205187559128,-0.008349043163304615,-0.0030640511041773216,0.0013973134731973653,0.007686961525385933]\n",
      "\n",
      "Text: [thank, @c3lshaded, \\ud83d\\udc99, mocha, looking, great!, , https://t.co/thgiimapre] => \n",
      "Vector: [-0.023882836831035092,0.003404416376724839,0.010953003307804465,-0.008934137411415577,-0.007306204759515822,0.004670743714086711,0.012038293876685202,0.004354041098849848,-0.0011021242244169116,0.004460028314497322]\n",
      "\n",
      "Text: [#junesploitation, day, 18:, free, space!, feature, #6:, eye, labyrinth, (1972), trailer:, , https://t.co/stxarecyav, , https://t.co/9lvyxwlman] => \n",
      "Vector: [-0.013389660037743549,0.002622538556655248,0.00958915421118339,0.012612517185819646,0.00465212557464838,-0.014866838689583044,0.0063303227070719,-0.016144459221201637,-0.014680908558269342,0.004246330633759498]\n",
      "\n",
      "Text: [never, maria, #magdalena,, , (you'e, creature, night), , maria, magdalena, , (you're, victim, fight), (you, need, love), , promise, delight,, , (you, need, love), \\u266a\\u266b., #sandra] => \n",
      "Vector: [0.007017136497709614,-0.0011653290577949241,0.003605579885725792,0.007083378324750811,0.0008935409580142453,-0.0023920311270138393,-0.0019316561475324517,-0.011528558402250593,-0.009115524048236415,0.0025697793596639084]\n",
      "\n",
      "Text: [#sarah, #cate, #sandra:, dream, almost, every, night., , https://t.co/rxmqgqwpdl] => \n",
      "Vector: [0.0018172206150160895,-0.020727276491622128,-0.00086360268566447,0.014854649185306495,0.0040584438635657225,-0.004485770516718427,-0.013474078823087944,-0.00793761227072941,0.0029397982741809553,0.006001408173081775]\n",
      "\n",
      "Text: [#sandra, going, leave, there., wise, choice!, $168,250, total., one-hundred-sixty-eight-and-a-quarter, grand., it\\u2019s, all., big., bucks., #pressyourluck] => \n",
      "Vector: [0.010618228527406851,0.006318699149414897,-0.002691758517175913,0.007124117047836383,0.012796650384552776,0.009526602077918748,-0.009963280940428376,-0.0019605768689264854,-0.012704035826027393,0.00010883496142923831]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing\n",
    "tokenizer = Tokenizer(inputCol='tweets', outputCol='tokens')\n",
    "df_tokens = tokenizer.transform(df).select('tokens')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='words')\n",
    "df_words = remover.transform(df_tokens).select('words')\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=10, minCount=0, inputCol=\"words\", outputCol=\"result\")\n",
    "model = word2Vec.fit(df_words)\n",
    "\n",
    "result = model.transform(df_words)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
